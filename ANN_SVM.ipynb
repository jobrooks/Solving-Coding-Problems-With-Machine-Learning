{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from csv \n",
    "kt_encoded = pd.read_csv(\"./encoded.csv\")\n",
    "\n",
    "# Scaling the features while removing results to another dataframe\n",
    "scaled_features = scaler.fit_transform(kt_encoded.drop(['accepted', 'wrong answer', 'error'], axis=1))\n",
    "scaled_features = pd.DataFrame(scaled_features, columns=kt_encoded.drop(['accepted', 'wrong answer', 'error'], axis=1).columns)\n",
    "\n",
    "result_encoded = kt_encoded[['accepted', 'wrong answer', 'error']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_features\n",
    "y = result_encoded\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y.values)\n",
    "\n",
    "# Combine X_train and y_train into a single DataFrame\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate minority and majority classes\n",
    "error_data = train_data[train_data['error'] == 1]\n",
    "accepted_data = train_data[train_data['accepted'] == 1]\n",
    "wrong_data = train_data[train_data['wrong answer'] == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "error_data_upsampled = resample(error_data, random_state=42, replace=True, n_samples=len(wrong_data))\n",
    "accepted_data_upsampled = resample(accepted_data, random_state=42, replace=True, n_samples=len(wrong_data))\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_data = pd.concat([error_data_upsampled, accepted_data_upsampled, wrong_data])\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train_upsampled = upsampled_data.drop(['accepted', 'wrong answer' , 'error'], axis=1)\n",
    "y_train_upsampled = upsampled_data[['accepted','wrong answer','error']].values.argmax(axis=1)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=4)\n",
    "X_new = selector.fit_transform(X_train_upsampled.values, y_train_upsampled)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "Best Hyperparameters: {'C': 10, 'decision_function_shape': 'ovo', 'kernel': 'linear'}\n",
      "Cross-Validation Results (f1): [0.73684211 0.57894737 0.68421053 0.57894737 0.57894737 0.52631579\n",
      " 0.5        0.61111111 0.5        0.61111111]\n",
      "Mean Accuracy: 0.5906432748538011\n",
      "SelectKBest SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72        11\n",
      "           1       1.00      0.33      0.50        15\n",
      "           2       0.43      0.86      0.57         7\n",
      "\n",
      "    accuracy                           0.61        33\n",
      "   macro avg       0.69      0.67      0.60        33\n",
      "weighted avg       0.76      0.61      0.59        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters you want to tune\n",
    "param_grid = [\n",
    "  { 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'kernel': ['linear'], 'decision_function_shape':['ovo','ovr']},\n",
    "  { 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf'],'decision_function_shape':['ovo','ovr']},\n",
    " ]\n",
    "\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm.SVC(),\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Print progress\n",
    "    scoring='f1_micro'  # Use accuracy as the evaluation metric, if imbalanced use f1 score instead\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_new, y_train_upsampled)\n",
    "\n",
    "# Get the best hyperparameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model found by GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "cross_val_results = cross_val_score(best_model, X_new, y_train_upsampled, cv=kfold)\n",
    "   \n",
    "print(f'Cross-Validation Results (f1): {cross_val_results}')\n",
    "print(f'Mean Accuracy: {cross_val_results.mean()}')\n",
    "\n",
    "X_test_selected = selector.fit_transform(X_test, y_test.values.argmax(axis=1))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_test.values.argmax(axis=1), y_pred, zero_division=1)\n",
    "print(\"SelectKBest SVM Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (40, 10), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (f1): [0.63157895 0.68421053 0.78947368 0.84210526 0.78947368 0.57894737\n",
      " 0.55555556 0.61111111 0.83333333 0.61111111]\n",
      "Mean Accuracy: 0.6926900584795321\n",
      "SelectKBest ANN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74        11\n",
      "           1       0.71      0.67      0.69        15\n",
      "           2       0.55      0.86      0.67         7\n",
      "\n",
      "    accuracy                           0.70        33\n",
      "   macro avg       0.71      0.72      0.70        33\n",
      "weighted avg       0.73      0.70      0.70        33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters you want to tune\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(40,), (40, 10)], #(40,)\n",
    "    'activation' : ['logistic', 'tanh', 'relu'], #relu\n",
    "    'solver' : ['sgd', 'adam'], # adam\n",
    "    'alpha': [0.0001, 0.001, 0.01], #.0001\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'max_iter': [1000],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Print progress\n",
    "    scoring='f1_micro'  # Use accuracy as the evaluation metric, if imbalanced use f1 score instead\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_new, y_train_upsampled)\n",
    "\n",
    "# Get the best hyperparameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model found by GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "cross_val_results = cross_val_score(best_model, X_new, y_train_upsampled, cv=kfold)\n",
    "   \n",
    "print(f'Cross-Validation Results (f1): {cross_val_results}')\n",
    "print(f'Mean Accuracy: {cross_val_results.mean()}')\n",
    "\n",
    "X_test_selected = selector.fit_transform(X_test, y_test.values.argmax(axis=1))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_test.values.argmax(axis=1), y_pred, zero_division=1)\n",
    "print(\"SelectKBest ANN Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
