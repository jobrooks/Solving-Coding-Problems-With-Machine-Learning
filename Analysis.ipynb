{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import textstat\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block gives the comments in a formatted way which lets us convert to a dataframe\n",
    "def get_comments(filepath):\n",
    "    comments = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                comments.append(line)\n",
    "            else:\n",
    "                break \n",
    "    return comments\n",
    "\n",
    "def extract_info(comment):\n",
    "    # Regular expression to extract difficulty, result, and tests from comments\n",
    "    pattern = r\"#\\s+(\\d+\\.\\d+)\\s+(.+?)\\s+(\\d+/\\d+)\"\n",
    "\n",
    "    match = re.match(pattern, comment)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None\n",
    "\n",
    "def find_gpt_files(directory):\n",
    "    gpt_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'gpt.py':\n",
    "                gpt_files.append(os.path.join(root, file))\n",
    "    return gpt_files\n",
    "\n",
    "def save_comments_to_csv(comments, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Difficulty\", \"Result\", \"Tests\"])\n",
    "        for comment in comments:\n",
    "            info = extract_info(comment)\n",
    "            if info:\n",
    "                writer.writerow(info)\n",
    "\n",
    "def main():\n",
    "    kattis_dir = os.path.join(os.getcwd(), 'kattis')\n",
    "    gpt_files = find_gpt_files(kattis_dir)\n",
    "    all_comments = []\n",
    "\n",
    "    for filepath in gpt_files:\n",
    "        comments = get_comments(filepath)\n",
    "        all_comments.extend(comments)\n",
    "\n",
    "    if all_comments:\n",
    "        csv_filename = 'kattis_comments.csv'\n",
    "        save_comments_to_csv(all_comments, csv_filename)\n",
    "        print(f\"Comments saved to CSV file: {csv_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found in any gpt.py files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the comments csv\n",
    "df_kattis = pd.read_csv(\"./kattis_comments.csv\")\n",
    "df_kattis['Result'] = df_kattis['Result'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Creating more columns for splitting the tests \n",
    "df_kattis[['Correct', 'Total']] = df_kattis['Tests'].str.split('/',expand=True).astype(int)\n",
    "\n",
    "# Getting the count of each result at the difficulties\n",
    "grouped = df_kattis.groupby(['Difficulty','Result']).size().unstack(fill_value=0)\n",
    "# display(grouped)\n",
    "\n",
    "# Creating a stacked bar chart for each result at each difficulty\n",
    "ax = grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "ax.set_xlabel('Difficulty')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Outcome Count by Difficulty Level')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Outcome')\n",
    "plt.show()\n",
    "\n",
    "# making all errors to be a general error and plotting again\n",
    "df_kattis['Result'] = df_kattis['Result'].apply(lambda x: 'error' if isinstance(x, str) and x.lower().split()[-1] in ['error', 'exceeded', 'exception'] else x)\n",
    "\n",
    "# Getting the count of each result at the difficulties\n",
    "grouped = df_kattis.groupby(['Difficulty','Result']).size().unstack(fill_value=0)\n",
    "# Creating a stacked bar chart for each result at each difficulty\n",
    "ax = grouped.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "ax.set_xlabel('Difficulty')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Outcome Count by Difficulty Level')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Outcome')\n",
    "plt.show()\n",
    "# Figure out which feutures to have as inputs in a prediction model, Figure engineering. With what to get from the problem statements. Readability, frequency of words, etc. think of meaningful features. perform unsupervised learning, understanding the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kattis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get all problems\n",
    "def find_prompt_files(directory):\n",
    "    prompt_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if 'gpt_prompt' in file:  # Check if 'gpt_prompt' is in the file name\n",
    "                prompt_files.append(os.path.join(root, file))\n",
    "    return prompt_files\n",
    "\n",
    "def get_problem(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        problem = f.read().strip()\n",
    "    return [problem]\n",
    "\n",
    "kattis_dir = os.path.join(os.getcwd(), 'kattis')\n",
    "prompt_files = find_prompt_files(kattis_dir)\n",
    "all_problems = []\n",
    "\n",
    "for filepath in prompt_files:\n",
    "    comments = get_problem(filepath)\n",
    "    all_problems.extend(comments)\n",
    "\n",
    "all_problems[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Look into another filter for words that are important\n",
    "data = []\n",
    "terms = set()\n",
    "with open(\"programming_terms.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        # Split the line into terms using comma as delimiter\n",
    "        term_list = line.strip().lower().split(',')\n",
    "        # Add each term to the set\n",
    "        for term in term_list:\n",
    "            terms.add(term.strip())\n",
    "\n",
    "for problem in all_problems[:20]:\n",
    "        doc = nlp(problem)\n",
    "        # Extract keywords (nouns and verbs) excluding stopwords\n",
    "        keywords = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS\n",
    "                    and token.text.lower() in terms\n",
    "                    and token.pos_ not in ['SYM', 'PUNCT', 'SPACE', 'X']]\n",
    "        # Count the frequency of each keyword\n",
    "        keyword_freq = Counter(keywords)\n",
    "        # Print the top keywords and their frequencies\n",
    "        data.append(keyword_freq)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all textstat readability and find best features from them\n",
    "with open(\"output.csv\", mode=\"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"ARI\", \"DCR\", \"DCR_V2\", \"FRE\", \"FKG\", \"SMOG\", \"CLI\", \"LINSEAR\", \"GF\", \"Textstd\", \"LexCount\", \"Difficult Words\"])\n",
    "\n",
    "    for problem in all_problems:\n",
    "        ARI = textstat.automated_readability_index(problem)\n",
    "        DCR = textstat.dale_chall_readability_score(problem)\n",
    "        DCR_v2 = textstat.dale_chall_readability_score_v2(problem)\n",
    "        FRE = textstat.flesch_reading_ease(problem)\n",
    "        FKG = textstat.flesch_kincaid_grade(problem)\n",
    "        SMOG = textstat.smog_index(problem)\n",
    "        CLI = textstat.coleman_liau_index(problem)\n",
    "        LINSEAR = textstat.linsear_write_formula(problem)\n",
    "        GF = textstat.gunning_fog(problem)\n",
    "        txtstd = textstat.text_standard(problem, float_output=False)\n",
    "        lex_count = textstat.lexicon_count(problem, removepunct=True)\n",
    "        difficult_words = textstat.difficult_words(problem)\n",
    "        info = ARI, DCR, DCR_v2, FRE, FKG, SMOG, CLI, LINSEAR, GF, txtstd, lex_count, difficult_words\n",
    "        writer.writerow(info)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopt one-hot encoding for y values: wrong, accepted, error\n",
    "# cross-validation techniques\n",
    "# grid search: model training\n",
    "\n",
    "features = pd.read_csv(\"./output.csv\")\n",
    "features['Difficulty'] = df_kattis['Difficulty']\n",
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kattis-1BYb5w23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
